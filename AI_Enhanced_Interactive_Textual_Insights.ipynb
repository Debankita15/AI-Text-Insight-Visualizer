{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Set Up the Environment in Google Colab\n",
        "\n",
        "####a. Install Required Libraries"
      ],
      "metadata": {
        "id": "Fr3TgkrkpY5w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSRqYi1Nn6ed",
        "outputId": "8e0832ba-be79-4a1e-ac6d-06dbb012218b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.10/dist-packages (1.38.0)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.2.2)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit) (1.4)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.1.7)\n",
            "Requirement already satisfied: numpy<3,>=1.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.26.4)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (24.1)\n",
            "Requirement already satisfied: pandas<3,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.1.4)\n",
            "Requirement already satisfied: pillow<11,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (10.4.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.20.3)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (14.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (13.8.1)\n",
            "Requirement already satisfied: tenacity<9,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.12.2)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.1.43)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.3)\n",
            "Requirement already satisfied: watchdog<5,>=2.1.5 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.0.2)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (3.1.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.12.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.11)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2024.8.30)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (2.18.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.5)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (24.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.20.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.3.0->streamlit) (1.16.0)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.10/dist-packages (7.2.0)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: yake in /usr/local/lib/python3.10/dist-packages (0.4.8)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from yake) (0.9.0)\n",
            "Requirement already satisfied: click>=6.0 in /usr/local/lib/python3.10/dist-packages (from yake) (8.1.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from yake) (1.26.4)\n",
            "Requirement already satisfied: segtok in /usr/local/lib/python3.10/dist-packages (from yake) (1.5.11)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from yake) (3.3)\n",
            "Requirement already satisfied: jellyfish in /usr/local/lib/python3.10/dist-packages (from yake) (1.1.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from segtok->yake) (2024.9.11)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (7.0.4)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.10/dist-packages (0.17.1)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.10/dist-packages (from textblob) (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (4.66.5)\n",
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.10/dist-packages (1.9.3)\n",
            "Requirement already satisfied: numpy>=1.6.1 in /usr/local/lib/python3.10/dist-packages (from wordcloud) (1.26.4)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from wordcloud) (10.4.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from wordcloud) (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (5.24.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly) (8.5.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from plotly) (24.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (3.3)\n",
            "Requirement already satisfied: psycopg2 in /usr/local/lib/python3.10/dist-packages (2.9.9)\n",
            "Requirement already satisfied: SQLAlchemy in /usr/local/lib/python3.10/dist-packages (2.0.35)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy) (4.12.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy) (3.1.1)\n"
          ]
        }
      ],
      "source": [
        "# Install required libraries\n",
        "!pip install streamlit\n",
        "!pip install pyngrok\n",
        "!pip install transformers\n",
        "!pip install torch\n",
        "!pip install yake\n",
        "!pip install gensim\n",
        "!pip install nltk\n",
        "!pip install textblob\n",
        "!pip install wordcloud\n",
        "!pip install plotly\n",
        "!pip install networkx\n",
        "!pip install psycopg2 SQLAlchemy\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####b. Download NLTK Data\n",
        "\n",
        "Download the required NLTK data for text processing."
      ],
      "metadata": {
        "id": "bdCkJK5QpoAd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lKR0NOLEpmpM",
        "outputId": "4cef61b7-7f43-4b46-dacb-cc3c6a4eede3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2. Set Up ngrok for Hosting the Streamlit App\n",
        "####a. Install and Authenticate ngrok"
      ],
      "metadata": {
        "id": "UzAdv_9bp9NG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pyngrok\n"
      ],
      "metadata": {
        "id": "eJwRGqB5rukE",
        "outputId": "3b0bce16-d94c-462a-ea6b-0bb2c871441e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.10/dist-packages (7.2.0)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "ngrok_auth_token = \"your_ngrok_token\"\n",
        "!ngrok authtoken {ngrok_auth_token}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IR3KMbQ1qC4Q",
        "outputId": "18c0ad37-0ce8-4203-e08e-e5d1c303df2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####c. Create app.py"
      ],
      "metadata": {
        "id": "RvMR1fATu0PX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import sqlite3\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import plotly.express as px\n",
        "import plotly.graph_objs as go\n",
        "import torch\n",
        "from transformers import pipeline\n",
        "import yake\n",
        "from gensim import corpora, models\n",
        "import matplotlib.pyplot as plt\n",
        "from textblob import TextBlob\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import networkx as nx\n",
        "from io import StringIO\n",
        "import os\n",
        "\n",
        "# Ensure NLTK data is downloaded\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Initialize Hugging Face pipelines\n",
        "@st.cache(allow_output_mutation=True)\n",
        "def load_pipelines():\n",
        "    summarizer = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\")\n",
        "    sentiment_analyzer = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "    return summarizer, sentiment_analyzer\n",
        "\n",
        "summarizer, sentiment_analyzer = load_pipelines()\n",
        "\n",
        "# Initialize SQLite database\n",
        "conn = sqlite3.connect('text_data.db')\n",
        "c = conn.cursor()\n",
        "\n",
        "# Create table if not exists\n",
        "c.execute('''\n",
        "    CREATE TABLE IF NOT EXISTS texts (\n",
        "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "        title TEXT,\n",
        "        content TEXT,\n",
        "        summary TEXT,\n",
        "        themes TEXT,\n",
        "        sentiment TEXT\n",
        "    )\n",
        "''')\n",
        "conn.commit()\n",
        "\n",
        "# Function to insert data into the database\n",
        "def insert_data(title, content, summary, themes, sentiment):\n",
        "    c.execute('''\n",
        "        INSERT INTO texts (title, content, summary, themes, sentiment)\n",
        "        VALUES (?, ?, ?, ?, ?)\n",
        "    ''', (title, content, summary, themes, sentiment))\n",
        "    conn.commit()\n",
        "\n",
        "# Function to retrieve data\n",
        "def get_all_data():\n",
        "    c.execute('SELECT * FROM texts')\n",
        "    data = c.fetchall()\n",
        "    return data\n",
        "\n",
        "# Function for keyword extraction using YAKE\n",
        "def extract_keywords(text, max_keywords=5):\n",
        "    kw_extractor = yake.KeywordExtractor(lan=\"en\", n=1, dedupLim=0.9, top=max_keywords, features=None)\n",
        "    keywords = kw_extractor.extract_keywords(text)\n",
        "    return \", \".join([kw[0] for kw in keywords])\n",
        "\n",
        "# Function for theme extraction using LDA (Gensim)\n",
        "def extract_topics(texts, num_topics=3):\n",
        "    # Tokenization and cleaning\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    processed_texts = []\n",
        "    for doc in texts:\n",
        "        tokens = nltk.word_tokenize(doc.lower())\n",
        "        tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
        "        processed_texts.append(tokens)\n",
        "\n",
        "    # Create dictionary and corpus\n",
        "    dictionary = corpora.Dictionary(processed_texts)\n",
        "    corpus = [dictionary.doc2bow(text) for text in processed_texts]\n",
        "\n",
        "    # LDA model\n",
        "    lda_model = models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=10)\n",
        "\n",
        "    # Extract topics\n",
        "    topics = lda_model.print_topics(num_words=3)\n",
        "    theme_list = [topic[1] for topic in topics]\n",
        "    return \"; \".join(theme_list)\n",
        "\n",
        "# Streamlit App Layout\n",
        "st.title(\"AI-Enhanced Interactive Textual Insights and Visualization Platform\")\n",
        "st.write(\"Upload your textual data, analyze it with AI, and visualize the insights.\")\n",
        "\n",
        "# Sidebar for Uploading Text\n",
        "st.sidebar.header(\"Upload Text Data\")\n",
        "uploaded_file = st.sidebar.file_uploader(\"Choose a text file (TXT or CSV)\", type=[\"txt\", \"csv\"])\n",
        "\n",
        "if uploaded_file is not None:\n",
        "    # Read the uploaded file\n",
        "    if uploaded_file.type == \"text/csv\":\n",
        "        data = pd.read_csv(uploaded_file)\n",
        "        text_column = st.sidebar.selectbox(\"Select text column\", data.columns)\n",
        "        texts = data[text_column].astype(str).tolist()\n",
        "    else:\n",
        "        data = StringIO(uploaded_file.getvalue().decode(\"utf-8\"))\n",
        "        texts = data.read().split('\\n')\n",
        "\n",
        "    st.sidebar.write(f\"Number of texts uploaded: {len(texts)}\")\n",
        "\n",
        "    # Input for Title\n",
        "    title = st.sidebar.text_input(\"Enter a title for this batch of texts\", \"Uploaded Texts\")\n",
        "\n",
        "    if st.sidebar.button(\"Analyze and Save\"):\n",
        "        with st.spinner(\"Analyzing texts with AI...\"):\n",
        "            summaries = []\n",
        "            themes_list = []\n",
        "            sentiments = []\n",
        "\n",
        "            for text in texts:\n",
        "                if text.strip() == \"\":\n",
        "                    continue\n",
        "                # Generate Summary\n",
        "                try:\n",
        "                    summary = summarizer(text, max_length=60, min_length=25, do_sample=False)[0]['summary_text']\n",
        "                except Exception as e:\n",
        "                    summary = \"Summary unavailable.\"\n",
        "                summaries.append(summary)\n",
        "\n",
        "                # Extract Themes using Keyword Extraction and Topic Modeling\n",
        "                keywords = extract_keywords(text)\n",
        "                themes_list.append(keywords)\n",
        "\n",
        "                # Perform Sentiment Analysis\n",
        "                try:\n",
        "                    sentiment = sentiment_analyzer(text)[0]['label']\n",
        "                except Exception as e:\n",
        "                    sentiment = \"Unknown\"\n",
        "                sentiments.append(sentiment)\n",
        "\n",
        "                # Insert into database\n",
        "                insert_data(title, text, summary, keywords, sentiment)\n",
        "\n",
        "        st.success(\"Analysis complete and data saved to database!\")\n",
        "\n",
        "# Display Stored Data\n",
        "st.header(\"Stored Textual Data\")\n",
        "data = get_all_data()\n",
        "if data:\n",
        "    df = pd.DataFrame(data, columns=['ID', 'Title', 'Content', 'Summary', 'Themes', 'Sentiment'])\n",
        "    st.dataframe(df)\n",
        "\n",
        "    # Visualization Options\n",
        "    st.header(\"Visualizations\")\n",
        "\n",
        "    # Sentiment Distribution\n",
        "    st.subheader(\"Sentiment Distribution\")\n",
        "    sentiment_counts = df['Sentiment'].value_counts().reset_index()\n",
        "    sentiment_counts.columns = ['Sentiment', 'Count']\n",
        "    fig1 = px.pie(sentiment_counts, names='Sentiment', values='Count', title='Sentiment Distribution')\n",
        "    st.plotly_chart(fig1)\n",
        "\n",
        "    # Themes Word Cloud\n",
        "    st.subheader(\"Themes Word Cloud\")\n",
        "    all_themes = ' '.join(df['Themes'].tolist())\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color='white', stopwords=stop_words, collocations=False).generate(all_themes)\n",
        "    fig2, ax = plt.subplots(figsize=(15, 7.5))\n",
        "    ax.imshow(wordcloud, interpolation='bilinear')\n",
        "    ax.axis('off')\n",
        "    st.pyplot(fig2)\n",
        "\n",
        "    # Thematic Relationship Network\n",
        "    st.subheader(\"Thematic Relationship Network\")\n",
        "    # Split themes and create edges based on co-occurrence in texts\n",
        "    theme_edges = []\n",
        "    for themes in df['Themes']:\n",
        "        theme_list = [theme.strip().lower() for theme in themes.split(',') if theme.strip() != '']\n",
        "        for i in range(len(theme_list)):\n",
        "            for j in range(i+1, len(theme_list)):\n",
        "                theme_edges.append((theme_list[i], theme_list[j]))\n",
        "\n",
        "    # Count edge frequencies\n",
        "    edge_df = pd.DataFrame(theme_edges, columns=['Theme1', 'Theme2'])\n",
        "    edge_counts = edge_df.groupby(['Theme1', 'Theme2']).size().reset_index(name='Count')\n",
        "\n",
        "    if not edge_counts.empty:\n",
        "        # Create a graph\n",
        "        G = nx.from_pandas_edgelist(edge_counts, 'Theme1', 'Theme2', ['Count'])\n",
        "\n",
        "        # Create positions\n",
        "        pos = nx.spring_layout(G, k=0.5, iterations=20)\n",
        "\n",
        "        # Create edge traces\n",
        "        edge_x = []\n",
        "        edge_y = []\n",
        "        for edge in G.edges():\n",
        "            x0, y0 = pos[edge[0]]\n",
        "            x1, y1 = pos[edge[1]]\n",
        "            edge_x.extend([x0, x1, None])\n",
        "            edge_y.extend([y0, y1, None])\n",
        "\n",
        "        edge_trace = go.Scatter(\n",
        "            x=edge_x, y=edge_y,\n",
        "            line=dict(width=0.5, color='#888'),\n",
        "            hoverinfo='none',\n",
        "            mode='lines')\n",
        "\n",
        "        # Create node traces\n",
        "        node_x = []\n",
        "        node_y = []\n",
        "        for node in G.nodes():\n",
        "            x, y = pos[node]\n",
        "            node_x.append(x)\n",
        "            node_y.append(y)\n",
        "\n",
        "        node_trace = go.Scatter(\n",
        "            x=node_x, y=node_y,\n",
        "            mode='markers',\n",
        "            hoverinfo='text',\n",
        "            marker=dict(\n",
        "                showscale=True,\n",
        "                colorscale='YlGnBu',\n",
        "                size=[5 + G.degree(node)*2 for node in G.nodes()],\n",
        "                color=[G.degree(node) for node in G.nodes()],\n",
        "                colorbar=dict(\n",
        "                    thickness=15,\n",
        "                    title='Degree',\n",
        "                    xanchor='left',\n",
        "                    titleside='right'\n",
        "                ),\n",
        "                line_width=2))\n",
        "\n",
        "        node_text = []\n",
        "        for node in G.nodes():\n",
        "            node_text.append(f\"{node} (Degree: {G.degree(node)})\")\n",
        "        node_trace.text = node_text\n",
        "\n",
        "        # Create figure\n",
        "        fig3 = go.Figure(data=[edge_trace, node_trace],\n",
        "                     layout=go.Layout(\n",
        "                        title='Thematic Relationship Network',\n",
        "                        titlefont_size=16,\n",
        "                        showlegend=False,\n",
        "                        hovermode='closest',\n",
        "                        margin=dict(b=20,l=5,r=5,t=40),\n",
        "                        annotations=[ dict(\n",
        "                            text=\"AI-Enhanced Interactive Textual Insights and Visualization Platform\",\n",
        "                            showarrow=False,\n",
        "                            xref=\"paper\", yref=\"paper\",\n",
        "                            x=0.005, y=-0.002 ) ],\n",
        "                        xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
        "                        yaxis=dict(showgrid=False, zeroline=False, showticklabels=False))\n",
        "                        )\n",
        "\n",
        "        st.plotly_chart(fig3)\n",
        "    else:\n",
        "        st.write(\"Not enough data to generate Thematic Relationship Network.\")\n",
        "\n",
        "    # Interactive Q&A with AI\n",
        "    st.header(\"Ask Questions About Your Texts\")\n",
        "    user_question = st.text_input(\"Enter your question:\")\n",
        "    if st.button(\"Get Answer\"):\n",
        "        if user_question.strip() == \"\":\n",
        "            st.write(\"Please enter a valid question.\")\n",
        "        else:\n",
        "            with st.spinner(\"Generating answer...\"):\n",
        "                # Since we are not using a generative model, we'll use keyword matching or predefined responses\n",
        "                # For more advanced Q&A, consider integrating with a local LLM if possible\n",
        "                # Here, we'll provide a placeholder response\n",
        "                st.write(\"**Answer:** This feature requires a generative model. Currently, it's not implemented.\")\n",
        "else:\n",
        "    st.write(\"Upload a text file to begin analysis.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jH5EjaXauxZ-",
        "outputId": "25d980a4-0712-4eca-f218-39fb5a6a5b74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###b. Function to run Streamlit app"
      ],
      "metadata": {
        "id": "jx40WHSxrH4b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "from threading import Thread\n",
        "import os\n",
        "import time\n",
        "import logging\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def manage_ngrok_tunnels(max_tunnels=2):\n",
        "    \"\"\"\n",
        "    Manages ngrok tunnels by ensuring that the number of active tunnels does not exceed max_tunnels.\n",
        "    If the limit is reached, the oldest tunnels are terminated to make room for new ones.\n",
        "    \"\"\"\n",
        "    tunnels = ngrok.get_tunnels()\n",
        "    active_tunnels = len(tunnels)\n",
        "\n",
        "    if active_tunnels >= max_tunnels:\n",
        "        # Number of tunnels to terminate\n",
        "        num_to_kill = active_tunnels - max_tunnels + 1\n",
        "        logger.info(f\"Active tunnels ({active_tunnels}) >= max_tunnels ({max_tunnels}). Terminating {num_to_kill} tunnel(s).\")\n",
        "\n",
        "        for i in range(num_to_kill):\n",
        "            tunnel = tunnels[i]\n",
        "            try:\n",
        "                ngrok.disconnect(tunnel.public_url)\n",
        "                logger.info(f\"Terminated tunnel: {tunnel.public_url}\")\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Failed to terminate tunnel {tunnel.public_url}: {e}\")\n",
        "\n",
        "def run_app():\n",
        "    \"\"\"\n",
        "    Runs the Streamlit app.\n",
        "    \"\"\"\n",
        "    os.system(\"streamlit run app.py --server.port 8501\")\n",
        "\n",
        "# Manage existing tunnels before starting a new one\n",
        "manage_ngrok_tunnels(max_tunnels=2)\n",
        "\n",
        "# Start the Streamlit app in a separate thread\n",
        "app_thread = Thread(target=run_app)\n",
        "app_thread.start()\n",
        "\n",
        "# Allow some time for the Streamlit app to start\n",
        "time.sleep(5)\n",
        "\n",
        "# Create a new ngrok tunnel for the Streamlit app\n",
        "try:\n",
        "    public_url = ngrok.connect(addr=8501, proto=\"http\")\n",
        "    logger.info(f\"🔗 Streamlit app is live at: {public_url}\")\n",
        "    print(f\"🔗 Streamlit app is live at: {public_url}\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"Failed to create ngrok tunnel: {e}\")\n",
        "    print(f\"❌ Failed to create ngrok tunnel: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ObjB3g9zrFfm",
        "outputId": "8f457540-def8-44b3-caa2-ffefb6c0c881"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2024-09-27T06:33:39+0000 lvl=warn msg=\"Stopping forwarder\" name=http-8501-a94d6e44-5974-4b24-ae04-35745bddf573 acceptErr=\"failed to accept connection: Listener closed\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔗 Streamlit app is live at: NgrokTunnel: \"https://5dd2-34-106-243-49.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    }
  ]
}